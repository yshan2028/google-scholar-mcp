"""
Google Scholar MCP Server with Multiple API Support
æ”¯æŒå¤šç§ API è®¿é—® Google Scholarï¼šScrapingDogã€SerpAPIã€scholarly
æä¾›ç¨³å®šå¯é çš„å­¦æœ¯æœç´¢æœåŠ¡
"""

from typing import Any, List, Dict, Optional
import asyncio
import logging
import os
import requests
from mcp.server.fastmcp import FastMCP

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Initialize FastMCP server
mcp = FastMCP("google-scholar-api")

# æ£€æŸ¥æ˜¯å¦å®‰è£…äº†å¿…è¦çš„åº“
try:
    from langchain_community.tools.google_scholar import GoogleScholarQueryRun
    from langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper
    SERPAPI_AVAILABLE = True
except ImportError:
    SERPAPI_AVAILABLE = False
    logging.warning("SerpAPI libraries not available. Please install: pip install google-search-results langchain-community")

# å¤‡ç”¨ï¼šä½¿ç”¨ scholarly åº“
try:
    from scholarly import scholarly
    SCHOLARLY_AVAILABLE = True
except ImportError:
    SCHOLARLY_AVAILABLE = False
    logging.warning("Scholarly library not available. Please install: pip install scholarly")


def get_scrapingdog_api_key() -> Optional[str]:
    """ä»ç¯å¢ƒå˜é‡è·å– ScrapingDog API å¯†é’¥"""
    return os.environ.get("SCRAPINGDOG_API_KEY")


def get_serp_api_key() -> Optional[str]:
    """ä»ç¯å¢ƒå˜é‡è·å– SerpAPI å¯†é’¥"""
    return os.environ.get("SERP_API_KEY")


async def search_with_scrapingdog(
    query: str, 
    num_results: int = 10, 
    language: str = "en",
    year_start: Optional[int] = None,
    year_end: Optional[int] = None
) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨ ScrapingDog API æœç´¢ Google Scholar
    å‚è€ƒæ–‡æ¡£: https://docs.scrapingdog.com/google-scholar-api
    
    Args:
        query: æœç´¢å…³é”®è¯
        num_results: è¿”å›ç»“æœæ•°é‡
        language: è¯­è¨€ï¼ˆé»˜è®¤è‹±æ–‡ï¼‰
        year_start: èµ·å§‹å¹´ä»½ï¼ˆå¯é€‰ï¼‰
        year_end: ç»“æŸå¹´ä»½ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        è®ºæ–‡åˆ—è¡¨ï¼ˆåŒ…å«å®Œæ•´å­—æ®µï¼‰
    """
    api_key = get_scrapingdog_api_key()
    if not api_key:
        raise ValueError("SCRAPINGDOG_API_KEY not found")
    
    url = "https://api.scrapingdog.com/google_scholar"
    params = {
        "api_key": api_key,
        "query": query,
        "language": language,
        "page": 0,
        "results": num_results
    }
    
    # æ·»åŠ å¹´ä»½è¿‡æ»¤ï¼ˆå¦‚æœæä¾›ï¼‰
    if year_start:
        params["as_ylo"] = str(year_start)
    if year_end:
        params["as_yhi"] = str(year_end)
    
    try:
        response = await asyncio.to_thread(requests.get, url, params=params, timeout=30)
        
        if response.status_code == 200:
            data = response.json()
            
            # è§£æ ScrapingDog è¿”å›çš„æ•°æ®ï¼ˆæ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼‰
            results = []
            organic_results = data.get('organic_results', [])
            
            for item in organic_results[:num_results]:
                # æå–ä½œè€…å’Œå¹´ä»½ï¼ˆä» displayed_linkï¼‰
                displayed_link = item.get('displayed_link', '')
                authors, year, venue = parse_displayed_link(displayed_link)
                
                # æå–å¼•ç”¨æ¬¡æ•°ï¼ˆä»å­—ç¬¦ä¸² "Cited by 1683" ä¸­æå–æ•°å­—ï¼‰
                citations = 0
                cited_by_text = item.get('inline_links', {}).get('cited_by', {}).get('total', '')
                if cited_by_text and isinstance(cited_by_text, str):
                    import re
                    match = re.search(r'\d+', cited_by_text)
                    if match:
                        citations = int(match.group())
                
                # æå–æ‰€æœ‰ PDF é“¾æ¥
                pdf_links = []
                resources = item.get('resources', [])
                for resource in resources:
                    if resource.get('type') == 'PDF':
                        pdf_links.append(resource.get('link', ''))
                
                # æ„å»ºç»“æœ
                result_data = {
                    'title': item.get('title', 'N/A'),
                    'title_link': item.get('title_link', 'N/A'),
                    'id': item.get('id', 'N/A'),
                    'type': item.get('type', ''),  # å¦‚ [BOOK], [PDF] ç­‰
                    'authors': authors,
                    'year': year,
                    'venue': venue,
                    'snippet': item.get('snippet', 'N/A'),  # è¿™æ˜¯æ‘˜è¦
                    'abstract': item.get('snippet', 'N/A'),  # åˆ«åï¼Œæ–¹ä¾¿ä½¿ç”¨
                    'citations': citations,
                    'url': item.get('title_link', 'N/A'),
                    'pdf_link': pdf_links[0] if pdf_links else 'N/A',
                    'all_pdf_links': pdf_links,
                    'inline_links': item.get('inline_links', {}),
                    'source': 'ScrapingDog'
                }
                results.append(result_data)
            
            return results
        else:
            raise Exception(f"ScrapingDog API request failed with status code: {response.status_code}")
            
    except Exception as e:
        logging.error(f"ScrapingDog API error: {str(e)}")
        raise


def parse_displayed_link(displayed_link: str) -> tuple:
    """
    è§£æ displayed_link å­—æ®µä»¥æå–ä½œè€…ã€å¹´ä»½å’ŒæœŸåˆŠ/ä¼šè®®
    ä¾‹å¦‚: "A Vaswani, N Shazeer - Advances in neural information processing systems, 2017"
    
    Returns:
        (authors, year, venue)
    """
    if not displayed_link or displayed_link == 'N/A':
        return ('N/A', 'N/A', 'N/A')
    
    try:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æ
        import re
        
        # å°è¯•åŒ¹é…æ ¼å¼: "ä½œè€… - æœŸåˆŠ/ä¼šè®®, å¹´ä»½ - æ¥æº"
        # æˆ–: "ä½œè€… - å¹´ä»½ - æ¥æº"
        parts = displayed_link.split(' - ')
        
        if len(parts) >= 2:
            authors = parts[0].strip()
            
            # æå–å¹´ä»½ï¼ˆ4ä½æ•°å­—ï¼‰
            year = 'N/A'
            venue = 'N/A'
            
            remaining = ' - '.join(parts[1:])
            year_match = re.search(r'\b(19|20)\d{2}\b', remaining)
            
            if year_match:
                year = year_match.group()
                # æå–æœŸåˆŠ/ä¼šè®®ï¼ˆå¹´ä»½ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
                venue_match = remaining[:year_match.start()].strip().rstrip(',').strip()
                if venue_match:
                    venue = venue_match
            else:
                # å¦‚æœæ²¡æœ‰å¹´ä»½ï¼Œç¬¬äºŒéƒ¨åˆ†å¯èƒ½æ˜¯æœŸåˆŠ/ä¼šè®®
                venue = parts[1].strip()
            
            return (authors, year, venue)
        else:
            return (displayed_link, 'N/A', 'N/A')
            
    except Exception as e:
        logging.warning(f"Failed to parse displayed_link: {e}")
        return (displayed_link, 'N/A', 'N/A')


@mcp.tool()
async def search_google_scholar(
    query: str, 
    num_results: int = 5,
    use_api: bool = True,
    language: str = "en"
) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨ Google Scholar æœç´¢å­¦æœ¯è®ºæ–‡
    æ™ºèƒ½é€‰æ‹©æœ€ä½³ APIï¼šScrapingDog -> SerpAPI -> scholarly
    
    Args:
        query: æœç´¢å…³é”®è¯
        num_results: è¿”å›ç»“æœæ•°é‡ (é»˜è®¤: 5)
        use_api: æ˜¯å¦ä½¿ç”¨ API (éœ€è¦ API key)ï¼Œå¦åˆ™ä½¿ç”¨ scholarly åº“
        language: è¯­è¨€ä»£ç  (é»˜è®¤: en)
    
    Returns:
        åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
    """
    logging.info(f"Searching Google Scholar for: {query}, num_results: {num_results}, use_api: {use_api}")
    
    if use_api:
        # ä¼˜å…ˆå°è¯• ScrapingDog APIï¼ˆå¦‚æœæœ‰ API Keyï¼‰
        scrapingdog_key = get_scrapingdog_api_key()
        if scrapingdog_key:
            try:
                logging.info("Using ScrapingDog API for search")
                results = await search_with_scrapingdog(query, num_results, language)
                logging.info(f"âœ… ScrapingDog API returned {len(results)} results")
                return results
            except Exception as e:
                logging.warning(f"ScrapingDog API failed: {str(e)}, trying next method...")
        
        # å¤‡é€‰ï¼šå°è¯• SerpAPI
        if SERPAPI_AVAILABLE:
            serp_key = get_serp_api_key()
            if serp_key:
                try:
                    logging.info("Using SerpAPI for search")
                    wrapper = GoogleScholarAPIWrapper()
                    tool = GoogleScholarQueryRun(api_wrapper=wrapper)
                    
                    result = await asyncio.to_thread(tool.run, query)
                    
                    if isinstance(result, str):
                        papers = []
                        lines = result.split('\n\n')
                        for i, line in enumerate(lines[:num_results]):
                            if line.strip():
                                papers.append({
                                    "title": f"Result {i+1}",
                                    "snippet": line.strip(),
                                    "source": "SerpAPI"
                                })
                        logging.info(f"âœ… SerpAPI returned {len(papers)} results")
                        return papers
                    else:
                        return [{"result": str(result), "source": "SerpAPI"}]
                        
                except Exception as e:
                    logging.warning(f"SerpAPI failed: {str(e)}, trying next method...")
    
    # æœ€åå¤‡ç”¨ï¼šä½¿ç”¨ scholarly åº“ï¼ˆå…è´¹ï¼‰
    if SCHOLARLY_AVAILABLE:
        try:
            logging.info("Using scholarly library for search (free fallback)")
            search_query = scholarly.search_pubs(query)
            results = []
            
            count = 0
            for pub in search_query:
                if count >= num_results:
                    break
                
                bib = pub.get('bib', {})
                result_data = {
                    'title': bib.get('title', 'N/A'),
                    'authors': bib.get('author', 'N/A'),
                    'year': bib.get('pub_year', 'N/A'),
                    'venue': bib.get('venue', 'N/A'),
                    'abstract': bib.get('abstract', 'N/A'),
                    'citations': pub.get('num_citations', 0),
                    'url': pub.get('pub_url', 'N/A'),
                    'source': 'scholarly'
                }
                results.append(result_data)
                count += 1
            
            logging.info(f"âœ… scholarly library returned {len(results)} results")
            return results
            
        except Exception as e:
            logging.error(f"Scholarly search failed: {str(e)}")
            return [{"error": f"All search methods failed. Last error: {str(e)}"}]
    
    return [{"error": "No search method available. Please install required libraries or configure API keys."}]


@mcp.tool()
async def search_google_scholar_by_author(
    author_name: str,
    query: Optional[str] = None,
    num_results: int = 5
) -> List[Dict[str, Any]]:
    """
    æŒ‰ä½œè€…æœç´¢ Google Scholar è®ºæ–‡
    
    Args:
        author_name: ä½œè€…å§“å
        query: å¯é€‰çš„å…³é”®è¯æŸ¥è¯¢
        num_results: è¿”å›ç»“æœæ•°é‡ (é»˜è®¤: 5)
    
    Returns:
        åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
    """
    logging.info(f"Searching papers by author: {author_name}, query: {query}")
    
    if not SCHOLARLY_AVAILABLE:
        return [{"error": "Scholarly library not available"}]
    
    try:
        # æ„å»ºæœç´¢æŸ¥è¯¢
        if query:
            search_str = f"{query} author:{author_name}"
        else:
            search_str = f"author:{author_name}"
        
        search_query = scholarly.search_pubs(search_str)
        results = []
        
        count = 0
        for pub in search_query:
            if count >= num_results:
                break
            
            bib = pub.get('bib', {})
            result_data = {
                'title': bib.get('title', 'N/A'),
                'authors': bib.get('author', 'N/A'),
                'year': bib.get('pub_year', 'N/A'),
                'venue': bib.get('venue', 'N/A'),
                'abstract': bib.get('abstract', 'N/A'),
                'citations': pub.get('num_citations', 0),
                'url': pub.get('pub_url', 'N/A')
            }
            results.append(result_data)
            count += 1
            
        return results
        
    except Exception as e:
        logging.error(f"Author search failed: {str(e)}")
        return [{"error": f"Author search failed: {str(e)}"}]


@mcp.tool()
async def get_author_profile(author_name: str) -> Dict[str, Any]:
    """
    è·å–ä½œè€…çš„è¯¦ç»†ä¿¡æ¯å’Œä¸»é¡µ
    
    Args:
        author_name: ä½œè€…å§“å
    
    Returns:
        åŒ…å«ä½œè€…ä¿¡æ¯çš„å­—å…¸
    """
    logging.info(f"Retrieving profile for author: {author_name}")
    
    if not SCHOLARLY_AVAILABLE:
        return {"error": "Scholarly library not available"}
    
    try:
        search_query = scholarly.search_author(author_name)
        author = await asyncio.to_thread(next, search_query)
        filled_author = await asyncio.to_thread(scholarly.fill, author)
        
        # æå–ç›¸å…³ä¿¡æ¯
        author_info = {
            "name": filled_author.get("name", "N/A"),
            "affiliation": filled_author.get("affiliation", "N/A"),
            "interests": filled_author.get("interests", []),
            "citedby": filled_author.get("citedby", 0),
            "citedby5y": filled_author.get("citedby5y", 0),
            "hindex": filled_author.get("hindex", 0),
            "hindex5y": filled_author.get("hindex5y", 0),
            "i10index": filled_author.get("i10index", 0),
            "i10index5y": filled_author.get("i10index5y", 0),
            "email": filled_author.get("email_domain", "N/A"),
            "homepage": filled_author.get("homepage", "N/A"),
            "top_publications": [
                {
                    "title": pub.get("bib", {}).get("title", "N/A"),
                    "year": pub.get("bib", {}).get("pub_year", "N/A"),
                    "citations": pub.get("num_citations", 0),
                    "venue": pub.get("bib", {}).get("venue", "N/A")
                }
                for pub in filled_author.get("publications", [])[:10]
            ]
        }
        
        return author_info
        
    except Exception as e:
        logging.error(f"Failed to retrieve author profile: {str(e)}")
        return {"error": f"Failed to retrieve author profile: {str(e)}"}


@mcp.tool()
async def search_google_scholar_advanced(
    query: str,
    author: Optional[str] = None,
    year_start: Optional[int] = None,
    year_end: Optional[int] = None,
    num_results: int = 5
) -> List[Dict[str, Any]]:
    """
    é«˜çº§æœç´¢ Google Scholarï¼ˆæ”¯æŒå¹´ä»½å’Œä½œè€…è¿‡æ»¤ï¼‰
    ä¼˜å…ˆä½¿ç”¨ ScrapingDog APIï¼Œæ”¯æŒå¹´ä»½å‚æ•°
    
    Args:
        query: æœç´¢å…³é”®è¯
        author: ä½œè€…åï¼ˆå¯é€‰ï¼‰
        year_start: èµ·å§‹å¹´ä»½ï¼ˆå¯é€‰ï¼ŒScrapingDog as_yloå‚æ•°ï¼‰
        year_end: ç»“æŸå¹´ä»½ï¼ˆå¯é€‰ï¼ŒScrapingDog as_yhiå‚æ•°ï¼‰
        num_results: è¿”å›ç»“æœæ•°é‡ (é»˜è®¤: 5)
    
    Returns:
        åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
    """
    logging.info(f"Advanced search - query: {query}, author: {author}, years: {year_start}-{year_end}")
    
    # ä¼˜å…ˆå°è¯• ScrapingDog APIï¼ˆæ”¯æŒå¹´ä»½å‚æ•°ï¼‰
    scrapingdog_key = get_scrapingdog_api_key()
    if scrapingdog_key:
        try:
            logging.info("Using ScrapingDog API for advanced search")
            
            # æ„å»ºæŸ¥è¯¢ï¼ˆæ·»åŠ ä½œè€…è¿‡æ»¤ï¼‰
            search_query = query
            if author:
                search_query += f" author:{author}"
            
            # ScrapingDog åŸç”Ÿæ”¯æŒå¹´ä»½è¿‡æ»¤ï¼ˆas_ylo, as_yhiï¼‰
            results = await search_with_scrapingdog(
                search_query, 
                num_results, 
                language="en",
                year_start=year_start,
                year_end=year_end
            )
            
            logging.info(f"âœ… ScrapingDog advanced search returned {len(results)} results")
            return results
            
        except Exception as e:
            logging.warning(f"ScrapingDog advanced search failed: {str(e)}, trying scholarly...")
    
    # å¤‡ç”¨ï¼šä½¿ç”¨ scholarly åº“
    if SCHOLARLY_AVAILABLE:
        try:
            logging.info("Using scholarly library for advanced search")
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_str = query
            if author:
                search_str += f" author:{author}"
            
            search_query = scholarly.search_pubs(search_str)
            results = []
            
            count = 0
            for pub in search_query:
                if count >= num_results:
                    break
                
                bib = pub.get('bib', {})
                
                # å¹´ä»½è¿‡æ»¤ï¼ˆscholarly éœ€è¦æ‰‹åŠ¨è¿‡æ»¤ï¼‰
                pub_year = bib.get('pub_year')
                if pub_year:
                    try:
                        pub_year_int = int(pub_year)
                        if year_start and pub_year_int < year_start:
                            continue
                        if year_end and pub_year_int > year_end:
                            continue
                    except (ValueError, TypeError):
                        pass
                
                result_data = {
                    'title': bib.get('title', 'N/A'),
                    'authors': bib.get('author', 'N/A'),
                    'year': pub_year or 'N/A',
                    'venue': bib.get('venue', 'N/A'),
                    'abstract': bib.get('abstract', 'N/A'),
                    'citations': pub.get('num_citations', 0),
                    'url': pub.get('pub_url', 'N/A'),
                    'source': 'scholarly'
                }
                results.append(result_data)
                count += 1
                
            return results
            
        except Exception as e:
            logging.error(f"Advanced search failed: {str(e)}")
            return [{"error": f"Advanced search failed: {str(e)}"}]
    
    return [{"error": "No search method available"}]


@mcp.tool()
async def search_google_scholar_key_words(query: str, num_results: int = 5) -> List[Dict[str, Any]]:
    """
    ã€å…¼å®¹æ€§åˆ«åã€‘æœç´¢ Google Scholarï¼ˆåŸç‰ˆå·¥å…·åï¼‰
    å®é™…è°ƒç”¨ search_google_scholar
    
    Args:
        query: æœç´¢å…³é”®è¯
        num_results: è¿”å›ç»“æœæ•°é‡ (é»˜è®¤: 5)
    
    Returns:
        åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
    """
    logging.info(f"[Compatibility] search_google_scholar_key_words called, forwarding to search_google_scholar")
    return await search_google_scholar(query, num_results, use_api=True)


@mcp.tool()
async def get_author_info(author_name: str) -> Dict[str, Any]:
    """
    ã€å…¼å®¹æ€§åˆ«åã€‘è·å–ä½œè€…ä¿¡æ¯ï¼ˆåŸç‰ˆå·¥å…·åï¼‰
    å®é™…è°ƒç”¨ get_author_profile
    
    Args:
        author_name: ä½œè€…å§“å
    
    Returns:
        åŒ…å«ä½œè€…ä¿¡æ¯çš„å­—å…¸
    """
    logging.info(f"[Compatibility] get_author_info called, forwarding to get_author_profile")
    return await get_author_profile(author_name)


@mcp.tool()
async def search_paper_by_title(paper_title: str) -> Dict[str, Any]:
    """
    é€šè¿‡è®ºæ–‡æ ‡é¢˜ç²¾ç¡®æœç´¢å¹¶è¿”å›å®Œæ•´å¼•ç”¨ä¿¡æ¯ï¼ˆç”¨äºè¡¥å…¨æ–‡çŒ®æ¡ç›®ï¼‰
    
    Args:
        paper_title: è®ºæ–‡æ ‡é¢˜
    
    Returns:
        åŒ…å«å®Œæ•´å¼•ç”¨ä¿¡æ¯çš„å­—å…¸ï¼ŒåŒ…æ‹¬ BibTeX æ ¼å¼
    """
    logging.info(f"Searching paper by exact title: {paper_title}")
    
    # å…ˆå°è¯•ä½¿ç”¨ ScrapingDog APIï¼ˆè¿”å›æ›´å®Œæ•´çš„æ•°æ®ï¼‰
    scrapingdog_key = get_scrapingdog_api_key()
    if scrapingdog_key:
        try:
            logging.info("Using ScrapingDog API to search by title")
            results = await search_with_scrapingdog(paper_title, num_results=1, language="en")
            
            if results and len(results) > 0:
                paper = results[0]
                
                # æ„å»ºå®Œæ•´çš„å¼•ç”¨ä¿¡æ¯
                citation_data = {
                    "title": paper.get('title', 'N/A'),
                    "authors": paper.get('authors', 'N/A'),
                    "year": paper.get('year', 'N/A'),
                    "abstract": paper.get('snippet', 'N/A'),
                    "citations": paper.get('citations', 0),
                    "url": paper.get('url', 'N/A'),
                    "pdf_link": paper.get('pdf_link', 'N/A'),
                    "source": "ScrapingDog"
                }
                
                # å°è¯•ä½¿ç”¨ scholarly è¡¥å……æ›´è¯¦ç»†çš„ä¿¡æ¯
                if SCHOLARLY_AVAILABLE:
                    try:
                        search_query = scholarly.search_pubs(paper_title)
                        scholarly_paper = await asyncio.to_thread(next, search_query)
                        
                        bib = scholarly_paper.get('bib', {})
                        citation_data.update({
                            "venue": bib.get('venue', citation_data.get('venue', 'N/A')),
                            "publisher": bib.get('publisher', 'N/A'),
                            "volume": bib.get('volume', 'N/A'),
                            "number": bib.get('number', 'N/A'),
                            "pages": bib.get('pages', 'N/A'),
                            "abstract_full": bib.get('abstract', citation_data.get('abstract', 'N/A')),
                            "eprint_url": scholarly_paper.get('eprint_url', 'N/A'),
                            "pub_url": scholarly_paper.get('pub_url', citation_data.get('url', 'N/A')),
                        })
                        
                        # ç”Ÿæˆ BibTeX
                        citation_data["bibtex"] = generate_bibtex(citation_data)
                        
                    except Exception as e:
                        logging.warning(f"Failed to get additional info from scholarly: {str(e)}")
                        citation_data["bibtex"] = generate_bibtex(citation_data)
                else:
                    citation_data["bibtex"] = generate_bibtex(citation_data)
                
                return citation_data
                
        except Exception as e:
            logging.warning(f"ScrapingDog search failed: {str(e)}, trying scholarly...")
    
    # ä½¿ç”¨ scholarly åº“
    if SCHOLARLY_AVAILABLE:
        try:
            logging.info("Using scholarly library to search by title")
            search_query = scholarly.search_pubs(paper_title)
            paper = await asyncio.to_thread(next, search_query)
            
            bib = paper.get('bib', {})
            
            citation_data = {
                "title": bib.get('title', 'N/A'),
                "authors": bib.get('author', 'N/A'),
                "year": bib.get('pub_year', 'N/A'),
                "venue": bib.get('venue', 'N/A'),
                "publisher": bib.get('publisher', 'N/A'),
                "volume": bib.get('volume', 'N/A'),
                "number": bib.get('number', 'N/A'),
                "pages": bib.get('pages', 'N/A'),
                "abstract": bib.get('abstract', 'N/A'),
                "citations": paper.get('num_citations', 0),
                "url": paper.get('pub_url', 'N/A'),
                "eprint_url": paper.get('eprint_url', 'N/A'),
                "source": "scholarly"
            }
            
            # ç”Ÿæˆ BibTeX
            citation_data["bibtex"] = generate_bibtex(citation_data)
            
            # ç”Ÿæˆ RIS æ ¼å¼
            citation_data["ris"] = generate_ris(citation_data)
            
            return citation_data
            
        except Exception as e:
            logging.error(f"Failed to search by title: {str(e)}")
            return {"error": f"Failed to find paper: {str(e)}"}
    
    return {"error": "No search method available"}


def generate_bibtex(citation_data: Dict[str, Any]) -> str:
    """
    ç”Ÿæˆ BibTeX æ ¼å¼çš„å¼•ç”¨
    """
    # ç”Ÿæˆ cite key (ä½¿ç”¨ç¬¬ä¸€ä¸ªä½œè€…å§“æ° + å¹´ä»½)
    authors = citation_data.get('authors', 'Unknown')
    year = citation_data.get('year', 'N/A')
    
    # æå–ç¬¬ä¸€ä¸ªä½œè€…çš„å§“æ°
    if isinstance(authors, str) and authors != 'N/A':
        first_author = authors.split(',')[0].strip().split()[-1]
    else:
        first_author = 'Unknown'
    
    cite_key = f"{first_author}{year}".replace(' ', '')
    
    # åˆ¤æ–­ç±»å‹ï¼ˆæ–‡ç« è¿˜æ˜¯ä¼šè®®è®ºæ–‡ï¼‰
    venue = citation_data.get('venue', '')
    entry_type = 'inproceedings' if 'conference' in venue.lower() or 'proceedings' in venue.lower() else 'article'
    
    bibtex = f"@{entry_type}{{{cite_key},\n"
    bibtex += f"  title = {{{citation_data.get('title', 'N/A')}}},\n"
    bibtex += f"  author = {{{citation_data.get('authors', 'N/A')}}},\n"
    bibtex += f"  year = {{{year}}},\n"
    
    if citation_data.get('venue', 'N/A') != 'N/A':
        if entry_type == 'article':
            bibtex += f"  journal = {{{citation_data.get('venue')}}},\n"
        else:
            bibtex += f"  booktitle = {{{citation_data.get('venue')}}},\n"
    
    if citation_data.get('volume', 'N/A') != 'N/A':
        bibtex += f"  volume = {{{citation_data.get('volume')}}},\n"
    
    if citation_data.get('number', 'N/A') != 'N/A':
        bibtex += f"  number = {{{citation_data.get('number')}}},\n"
    
    if citation_data.get('pages', 'N/A') != 'N/A':
        bibtex += f"  pages = {{{citation_data.get('pages')}}},\n"
    
    if citation_data.get('publisher', 'N/A') != 'N/A':
        bibtex += f"  publisher = {{{citation_data.get('publisher')}}},\n"
    
    if citation_data.get('url', 'N/A') != 'N/A':
        bibtex += f"  url = {{{citation_data.get('url')}}},\n"
    
    bibtex += "}\n"
    
    return bibtex


def generate_ris(citation_data: Dict[str, Any]) -> str:
    """
    ç”Ÿæˆ RIS æ ¼å¼çš„å¼•ç”¨
    """
    venue = citation_data.get('venue', '')
    ty = 'CONF' if 'conference' in venue.lower() or 'proceedings' in venue.lower() else 'JOUR'
    
    ris = f"TY  - {ty}\n"
    ris += f"TI  - {citation_data.get('title', 'N/A')}\n"
    
    # å¤„ç†ä½œè€…
    authors = citation_data.get('authors', 'N/A')
    if isinstance(authors, str) and authors != 'N/A':
        for author in authors.split(','):
            ris += f"AU  - {author.strip()}\n"
    
    ris += f"PY  - {citation_data.get('year', 'N/A')}\n"
    
    if citation_data.get('venue', 'N/A') != 'N/A':
        if ty == 'JOUR':
            ris += f"JO  - {citation_data.get('venue')}\n"
        else:
            ris += f"T2  - {citation_data.get('venue')}\n"
    
    if citation_data.get('volume', 'N/A') != 'N/A':
        ris += f"VL  - {citation_data.get('volume')}\n"
    
    if citation_data.get('number', 'N/A') != 'N/A':
        ris += f"IS  - {citation_data.get('number')}\n"
    
    if citation_data.get('pages', 'N/A') != 'N/A':
        ris += f"SP  - {citation_data.get('pages')}\n"
    
    if citation_data.get('abstract', 'N/A') != 'N/A':
        ris += f"AB  - {citation_data.get('abstract')}\n"
    
    if citation_data.get('url', 'N/A') != 'N/A':
        ris += f"UR  - {citation_data.get('url')}\n"
    
    ris += "ER  - \n"
    
    return ris


@mcp.tool()
async def get_citation_info(paper_title: str) -> Dict[str, Any]:
    """
    è·å–è®ºæ–‡çš„å¼•ç”¨ä¿¡æ¯
    
    Args:
        paper_title: è®ºæ–‡æ ‡é¢˜
    
    Returns:
        åŒ…å«å¼•ç”¨ä¿¡æ¯çš„å­—å…¸
    """
    logging.info(f"Getting citation info for: {paper_title}")
    
    if not SCHOLARLY_AVAILABLE:
        return {"error": "Scholarly library not available"}
    
    try:
        search_query = scholarly.search_pubs(paper_title)
        paper = await asyncio.to_thread(next, search_query)
        filled_paper = await asyncio.to_thread(scholarly.fill, paper)
        
        bib = filled_paper.get('bib', {})
        citation_info = {
            "title": bib.get('title', 'N/A'),
            "authors": bib.get('author', 'N/A'),
            "year": bib.get('pub_year', 'N/A'),
            "venue": bib.get('venue', 'N/A'),
            "citations": filled_paper.get('num_citations', 0),
            "url": filled_paper.get('pub_url', 'N/A'),
            "eprint_url": filled_paper.get('eprint_url', 'N/A'),
            "abstract": bib.get('abstract', 'N/A')
        }
        
        return citation_info
        
    except Exception as e:
        logging.error(f"Failed to get citation info: {str(e)}")
        return {"error": f"Failed to get citation info: {str(e)}"}


def main():
    """Entry point for setuptools and Docker"""
    # ç¯å¢ƒæ£€æŸ¥
    logging.info("=" * 60)
    logging.info("Google Scholar MCP Server - Environment Check")
    logging.info("=" * 60)
    
    # æ£€æŸ¥ ScrapingDog API
    scrapingdog_key = get_scrapingdog_api_key()
    if scrapingdog_key:
        logging.info(f"âœ… SCRAPINGDOG_API_KEY found: {scrapingdog_key[:10]}... (ä¼˜å…ˆä½¿ç”¨)")
    else:
        logging.info("âš ï¸  SCRAPINGDOG_API_KEY not configured")
    
    # æ£€æŸ¥ SerpAPI
    serp_key = get_serp_api_key()
    if serp_key:
        logging.info(f"âœ… SERP_API_KEY found: {serp_key[:10]}... (å¤‡é€‰)")
    else:
        logging.info("âš ï¸  SERP_API_KEY not configured")
    
    # æ£€æŸ¥åº“å¯ç”¨æ€§
    if SERPAPI_AVAILABLE:
        logging.info("âœ… SerpAPI libraries available")
    else:
        logging.info("âš ï¸  SerpAPI libraries not available")
    
    if SCHOLARLY_AVAILABLE:
        logging.info("âœ… Scholarly library available (å…è´¹å¤‡ç”¨)")
    else:
        logging.warning("âš ï¸  Scholarly library not available")
    
    # æ˜¾ç¤º API ä¼˜å…ˆçº§
    logging.info("")
    logging.info("API ä¼˜å…ˆçº§é¡ºåº:")
    logging.info("  1. ScrapingDog API (æœ€å¿«æœ€ç¨³å®š)")
    logging.info("  2. SerpAPI (å¤‡é€‰)")
    logging.info("  3. scholarly åº“ (å…è´¹ä½†è¾ƒæ…¢)")
    logging.info("")
    
    # å¯åŠ¨æœåŠ¡å™¨
    logging.info("=" * 60)
    logging.info("ğŸš€ Starting Google Scholar MCP Server...")
    logging.info("=" * 60)
    mcp.run()


if __name__ == "__main__":
    main()

